{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import nltk\r\n",
    "import numpy as np\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# downloading model to tokenize message\r\n",
    "nltk.download('punkt')\r\n",
    "# downloading stopwords\r\n",
    "nltk.download('stopwords')\r\n",
    "# downloading wordnet, which contains all lemmas of english language\r\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\sana\n",
      "[nltk_data]     khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\sana\n",
      "[nltk_data]     khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\sana\n",
      "[nltk_data]     khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "from nltk.corpus import stopwords\r\n",
    "stop_words = stopwords.words('english')\r\n",
    "\r\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "corpus = pd.read_csv(\"C:\\\\Users\\\\sana khan\\\\Desktop\\\\MLLLL\\\\BBC News Train.csv\")\n",
    "print(corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      ArticleId                                               Text  \\\n",
      "0          1833  worldcom ex-boss launches defence lawyers defe...   \n",
      "1           154  german business confidence slides german busin...   \n",
      "2          1101  bbc poll indicates economic gloom citizens in ...   \n",
      "3          1976  lifestyle  governs mobile choice  faster  bett...   \n",
      "4           917  enron bosses in $168m payout eighteen former e...   \n",
      "...         ...                                                ...   \n",
      "1485        857  double eviction from big brother model caprice...   \n",
      "1486        325  dj double act revamp chart show dj duo jk and ...   \n",
      "1487       1590  weak dollar hits reuters revenues at media gro...   \n",
      "1488       1587  apple ipod family expands market apple has exp...   \n",
      "1489        538  santy worm makes unwelcome visit thousands of ...   \n",
      "\n",
      "           Category  \n",
      "0          business  \n",
      "1          business  \n",
      "2          business  \n",
      "3              tech  \n",
      "4          business  \n",
      "...             ...  \n",
      "1485  entertainment  \n",
      "1486  entertainment  \n",
      "1487       business  \n",
      "1488           tech  \n",
      "1489           tech  \n",
      "\n",
      "[1490 rows x 3 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def clean_corpus(corpus):\n",
    "  # lowering every word in text\n",
    "  corpus = [ doc.lower() for doc in corpus]\n",
    "  cleaned_corpus = []\n",
    "  \n",
    "  stop_words = stopwords.words('english')\n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  # iterating over every text\n",
    "  for doc in corpus:\n",
    "    # tokenizing text\n",
    "    tokens = word_tokenize(doc)\n",
    "    cleaned_sentence = [] \n",
    "    for token in tokens: \n",
    "      # removing stopwords, and punctuation\n",
    "      if token not in stop_words and token.isalpha(): \n",
    "        # applying lemmatization\n",
    "        cleaned_sentence.append(wordnet_lemmatizer.lemmatize(token)) \n",
    "    cleaned_corpus.append(' '.join(cleaned_sentence))\n",
    "  return cleaned_corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cleaned_corpus\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}